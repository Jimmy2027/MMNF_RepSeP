\chapter{Related Work}
\section{Generative Modeling}
Generative adversarial networks \citep[GANs]{goodfellow_generative_2014} and variational autoencoders (VAEs, \cref{subsec:vae}) are the two most popular methods for generative modeling.
Both attempt to model the distribution over the data, however while for the VAEs, the resulting posterior approximation is defined explicitly, the learned posterior of GANs can not be evaluated directly.
GANs are made of two models that are trained simultaneously, a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.
The joint optimization of both models D and G can be tricky in practice and GANs are known to suffer from mode collapse since the objective does not require the learned representation to contain all modes of the data.
For images of animals for example, the generator G could learn to generate only images of brown, short haired dogs, so well that the discriminator D will not be able to distinguish them from the true data.
Mode collapse does not happen in VAEs since their objective explicitly requires their learned representation to contain all modes of the data.
Also, since the learned posterior distribution of VAEs can be evaluated explicitly, additional constraints can be added to the objective to push the posterior distribution to have specific characteristics and it can be used for downstream tasks like clustering or classification.
In this work, we focus on VAEs and give a more in depth introduction in section \cref{subsec:vae}.

\section{Multi Modal Generative Modeling}

There have been a wide range of approaches for multi-modal generative modeling, however most fall short of expressing the complete range of behaviour that we expect in this setting.

\paragraph{Modality Translation}
Most prior approaches to generative modelling with multi modal data have targeted modality translation, where the model learns to generate one modality conditioned on another one.
In this case input an output modalities of the model are not interchangeable.
Modality translation has been proposed both as VAE based \citep{pu2016variational, pandey2017variational}, as well as GAN based, for domain translation of images \citep{ledig2017photo, liu2019few}.
% todo
However, we expect our method to be able to generate any modality given any subset of modalities which extends translation between modalities.
It would be possible to train $2^M -1$ modality translation network pairs for $M$ modalities, but this is intractable in practice.

\paragraph{Joint approximation}
Other prior work has targeted to directly model the joint distribution over the data.
The joint multi modal VAE (JMVAE) from \citep{suzuki2016joint} learns a joint posterior distribution using a joint inference network.
To handle missing data at test time, inference networks need to be trained for every subset of modalities.
While feasible for two modalities, this setup quickly becomes intractable with more data types.
Similarly, the multimodal factorisation model (MFM) from \citep{tsai2018learning} explicitly defines a joint inference network on top of uni modal encoders, however additional decoder networks are needed to generate missing modalities.

These approaches typically do not scale well with the number of modalities since they require additional modelling components for each combination of modalities.
The MVAE from \citep{poe} marked an improvement over previous methods in this regard, proposing to model the joint posterior as a product of experts (POE) over the marginal posteriors, enabling cross-modal generation at test-time without requiring additional inference networks and multi-stage training regimes.
Since then, other methods have emerged, each proposing another aggregation function over the marginal posteriors.
We refer to \cref{subsec:Multi Modal VAEs} for a more in depth introduction to the MVAE and other methods that build on it.

Next to the aggregation function with which the uni modal posteriors are merged, other methods have been proposed to improve multi modal VAEs (mmVAEs).
In \citep{daun_disent}, the authors propose to split the latent space into modality specific and shared information in order to disentangle \citep{burgess_understanding_2018} them in a purely self-supervised manner.
The aggregation of modalities should only happen over the shared information and thus it makes sense to separate it from the modality specific information in order to simplify the aggregation.
For this, the authors add a new term to the mmVAE objective, which disentangles the shared representations with the modality specific representations and encourages mutual information between representations that contain shared information.
This has been shown to improve the conditional generation of missing modalities, however the results from \citep{sutter_multimodal_2020} point out that independent of that separation, the generation coherence differs between different merging functions.
The goal of this work is solely to improve the merging function, which is why we forgo this method even though we expect the separation of shared and modality specific information to improve our results.
