\section{Introduction}

The availability of multiple data types provides a rich source of information and holds promise for learning representations that generalise well across multiple modalities \parencite{baltrusaitis_multimodal_2019}.
Similar to how humans learn and extract information from their surrounding using their five senses, a machine learning model could learn from multiple data types.
Multimodal data naturally grants additional self-supervision in the form of shared information connecting the different data types.
Further, the understanding of different modalities and the interplay between data types are non-trivial research questions and longstanding goals in machine learning research.
While fully-supervised approaches have been applied successfully \parencite{karpathy_deep_2015,tsai_learning_2018}, the labeling of multiple data types remains time-consuming and expensive.
Therefore, it requires models that efficiently learn from multiple data types in a self-supervised fashion.
Self-supervised, generative models are suitable for learning the joint distribution of multiple data types without supervision.
Here, we focus on VAEs \parencite{kingma_auto-encoding_2014,rezende_stochastic_2014} which are able to jointly infer representations and generate new observations.
Despite their success on unimodal datasets, there are additional challenges associated with multimodal data \parencite{suzuki_joint_2016, vedantam_generative_2018}.
In particular, multimodal generative models need to represent both modality-specific and shared factors and generate semantically coherent samples across modalities.
Semantically coherent samples are connected by the information which is shared between data types \parencite{shi_variational_2019}.
These requirements are not inherent to the objective: the evidence lower bound (ELBO) of uni-modal VAEs.
Hence, adaptions to the original formulation are required to cater to and benefit from multiple data types.

% todo introduce that previous methods use the geometric or the arithmetic mean and that we are generalizing this here

Normalizing flows represent an approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions \parencite{papamakarios_normalizing_2019,rezende_variational_2016}.
They consist of transforming a simple initial density into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained.
For this project, I propose to use normalizing flows as a parameterisable function $f_\psi$ which combines the latent variables of multiple data types.
Normalizing flows are optimal for this task since they can transform simple initial densities into arbitrarily complex ones and vice versa.

Normalizing flows became very popular in the past years, and many methods already exist for different applications, however they have not yet been used to create a joint latent representation from multiple data types for VAEs.
The task in this project is to find an adequate implementation of normalizing flows, i.e. the best trade-off between the complexity of the resulting joint latent representation and the computational feasibility, and evaluate it on existing multimodal datasets.
To this end, I would first compare simpler methods, with a less good approximation of the true joint posterior but less computationally expensive, with more complex methods on toy datasets.
The ultimate goal being to construct a method that can learn a separable \footnote{separable in the sense that the joint latent representation can be separated into the different classes that span the dataset} joint latent representation from which coherent \footnote{coherent in the sense that all generated samples belong to the same class} samples can be generated on the challenging MIMIC-CXR \parencite{johnson_mimic-cxr-jpg_2019} database.