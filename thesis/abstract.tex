\begin{abstract}
%  Multimodal data naturally grants self-supervision in the form of shared information connecting the different data types.
    Multi modal, generative models are able to learn underlying generative factors of multiple data types without the need for supervision.
    Existing methods use a fixed, pre-selected aggregation function to merge the learned representation of each modality into a joint posterior distribution.
    Here, we generalise previous work by implementing the aggregation over modalities using a trainable generalized $f$-means.
    We show that this more flexible way to fuse the information between modalities improves the ability of the model to learn a meaningful joint posterior approximation and to generate coherent samples across data types.
%  We show that this more flexible way to fuse the information between modalities improves the ability of the model to generate coherent samples across modalities.
\end{abstract}
