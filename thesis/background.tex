\chapter{Background}
Our methods build on concepts and previous work on variational autoencoders (VAEs), self-supervised multi modal generative learning paradigms and normalizing flows, which we introduce in this section.


\section{Variational Autoencoder}
\label{subsec:vae}
The VAE, first introduced by \citep{kingma_auto-encoding_2014} and \citep{rezende_stochastic_2014}, consists of an encoder network and a decoder network.
In contrast to a typical auto encoder network, the VAE is trained such that its learned representation has the structure of a prior distribution.
The most popular choice for a prior is the standard Gaussian distribution $\mathcal{N}(0,\textbf{I})$, which we also use in this work.
The latent representation being a distribution, the decoder part can generate unseen data by sampling from it.
The model is trained such that it maximizes the log-likelihood of the data ($\log p(x)$) by maximizing the Evidence Lower BOund (ELBO):
\begin{equation}
    \label{vaeelbo}
    \begin{split}
        \log p(x) &= \log \int p(x,z) dz\\
        &=  \log \int p(x,z) \frac{q(z|x)}{q(z|x)}dz\\
        &\geq \mathbb{E} _{q(z|x)}[\log \frac{p(x|z)p(z)}{q(z|x)}]\\
        &= \mathbb{E} _{q(z|x)}[\log p(x|z)] - \mathbb{E} _{q(z|x)}[\log \frac{q(z|x)}{p(z)}]\\
        &= \mathbb{E} _{q(z|x)}[\log p(x|z)] - D_{KL}\left( q(z|x)\ ||\ p(z)\right)\\
        &= ELBO
    \end{split}
\end{equation}

The ELBO consists of two parts: the reconstruction loss which pushes the generated samples to resemble the real data and a regularization term which forces the latent representation to be structured like the prior.
In \citep{beta_vae}, the authors introduce the hyperparameter $\beta$, which allows to weight the regularization term in the VAE objective:
\begin{equation}
    \label{eq:vaeelbo}
    \mathcal{L}_{ELBO} = \mathbb{E} _{q(z|x)}[\log p(x|z)] - \beta D_{KL}\left( q(z|x)\ ||\ p(z)\right)
\end{equation}

A lower $\beta$ gives the model more freedom in learning the latent representation, while a higher $\beta$ forces the model to learn a latent distribution that is disentangled, like the prior.
"Disentangled" here means that each dimension in the learned latent representation is independent of each other, and represents a latent factor that corresponds to a different attribute in the data.
In images of animals for example, one dimension in the latent representation could represent the color of the fur, while another might correspond to the color of the eyes.
Both the color of the fur and the color of the eyes are independent, and so should be the corresponding latent variables.
A structured and disentangled representation leads to better interpretability and is widely believed to lead to better results in down-stream tasks, however this claim has been challenged in \citep{locatello_challenging_2019} where the authors could not find evidence for it.

The $\beta$ allows to weight the trade-off introduced by the modified training objective that punishes reconstruction quality in order to encourage disentanglement within the latent representations \citep{burgess_understanding_2018}.
Disentanglement is a popular objective in representation learning and has been addressed in recent works \parencite{chen_isolating_2019, locatello_challenging_2019}.
In this work, we also make use of $\beta$ as a hyperparameter that we adapt for each method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Multi Modal VAEs}
\label{subsec:Multi Modal VAEs}
In order for the VAE model to learn a representation which captures the underlying factors of multiple modalities, several adaptations to the objective in \cref{vaeelbo} need to be made.
The first approach that scales with the number of modalities, allows for a coherent joint generation over all modalities and cross-generation across individual modalities, the MVAE, was introduced in \citep{poe}.
The MVAE makes the assumption that the joint posterior of data containing M modalities $\mathbb{X} = \{ \xset _m\}_{m=1}^M$ is a product of uni modal posteriors, also called a Product-of-Experts (PoE) \parencite{hinton_training_2002}:

\begin{equation}
    p(z|\xset_1,\ldots,\xset_M) \propto \prod ^M _{m=1} q(z|\xset_m)
\end{equation}

The PoE has the advantage of aggregating information across any subset of uni modal posteriors which allows for missing modalities.
However, the product of experts does not train the individual inference networks and they don't learn to handle missing data at test time.
To address this issue, the MVAE requires a sub-sampling of uni modal log-likelihoods, which no longer guarantees a valid lower bound on the joint log-likelihood \parencite{wu_multimodal_2019}.

Another approach was proposed with the MMVAE in \citep{shi2019variational}, which models the joint posterior as a mixture of uni modal posteriors, i.e. a mixture of experts (MoE):

\begin{equation}
    p(z|\xset_1,\ldots,\xset_M) = \frac{1}{M} \sum _{m=1} q(z|\xset_m)
\end{equation}

The MoE has the advantage of optimizing each inference network individually, however it does not merge the information between posteriors since only uni modal posteriors are considered during training.

Both advantages of the PoE, which results in a good approximation of the joint distribution and the MoE which optimizes each uni modal posterior individually are combined in the \mg{MoPoE} \parencite{thomas_gener-ELBO}.
The \mg{MoPoE}-VAE takes advantage of both methods by merging the uni modal posteriors into $2^M - 1$ subsets, which are then combined with a MoE (see \cref{mopoeGraph}).

\begin{figure}[h!]
    \centering
    \resizebox{\textwidth}{!}{%
        \py{pytex_printonly(script='scripts_/mopoe_graph.py', data = '')}
    }
    \caption{\textbf{The \mg{MoPoE} makes use of the PoE to create $2^M$ subsets, which are then merged with a MoE.} Here $M=2$, the empty subset is not shown. On the left side are the two input modalities from the polymnist dataset (see \cref{polymnist}), on the right side are the generated samples. In the header of each generated sample is described from which subset the decoder sampled for the generation (left side of the $\rightarrow$) and which modality was generated (right side of the $\rightarrow$).}
    \label{mopoeGraph}
\end{figure}

Similar to the MoE, the \mg{MoPoE} models the joint posterior as a mixture.
However the mixture of experts consists of subsets instead of uni modal posteriors.
For multi modal data $\mathbb{X} = \{ \xset _m\}_{m=1}^M$ with M modalities, and $2^M - 1$ subsets of modalities $\mathbb{X}_s \in \mathbb{X}$, the objective of the \mg{MoPoE}, which is an evidence lower bound (ELBO) on the joint log-likelihood $\log p_{\theta}(\xset)$, can be written as follows:

\begin{equation}
    \label{eq:mopoe}
    \eqlmopoe
\end{equation}

with $\jointpost$ the joint posterior:
\begin{equation}
    \jointpost = \frac{1}{2^M} \sum _{\xsubset \in \powerset} \subsetpost
\end{equation}

and $\subsetpost$ the posterior approximation of subset $\xsubset$:
\begin{equation}
    \label{eq:subsetmopoe}
    \subsetpost=PoE(\{\unimodalpost \forall \xsetm \in \xsubset\}) \propto \prod _{\xsetm \in \xsubset}\unimodalpost)
\end{equation}

For gaussian posteriors, the PoE in \cref{eq:subsetmopoe} can be computed in closed form.
\Cref{lemma:DklLowerBound} from \citep{sutter_multimodal_2020} states that the KL-divergence of the multimodal posterior distribution is a lower bound for the weighted sum of the KL-divergences of the unimodal variational approximation functions.
Accordingly, \Cref{eq:mopoe} can be further simplified:

\begin{equation}
    \label{eq:mopoe_}
    \lmopoe \leq \mathbb{E}_{q_{\phi}(\textbf{z}|\mathbb{X})}[\log (p_{\theta}(\mathbb{X}|\textbf{z}))] - \frac{1}{2^M} \sum _{\mathbb{X}_s \in \mathcal{P}(\mathbb{X})} D_{KL}\biggl( \tilde{q}_{\phi}(\textbf{z}|\mathbb{X}_s)\ ||\ p_{\theta}(\textbf{z})\biggr)
\end{equation}


It has been shown in \citep{sutter_multimodal_2020} that the joint generation coherence of the \mg{MoE} surpasses that of the \mg{MoPoE}, suggesting that a more flexible aggregation function might be needed to further improve results.
In this work, the \mg{MoPoE} is taken as the current state of the art for scalable, self-supervised, multi modal generative models and is used as baseline to compare our methods against.
We also compare to the \mg{PoE} which is seen as the gold standard for aggregating information across modalities and the \mg{MoE} for learning each modality equally well and obtaining an informative joint posterior.


\begin{lemma}[Joint Approximation Function]
    \label{lemma:DklLowerBound}

    The KL-divergence of the multimodal variational posterior approximation is a lower bound for the weighted sum of the KL-divergences of the unimodal variational approximation functions \citep{sutter_multimodal_2020}:
    \begin{equation}
        D_{KL} \left( \sum _{i=1} ^M \frac{1}{M}\unimodalpost\ ||\ \prior \right) \leq \sum _{i=1} ^M \frac{1}{M} D_{KL} \left( \unimodalpost\ ||\ \prior \right)
    \end{equation}
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Normalizing Flows}
\label{subsec: Normalizing Flows}
Normalizing flows \parencite{papamakarios_normalizing_2019} represent an approach for defining invertible and differentiable transformations of probability distributions.
They are widely used for generative modeling \citep{dinh_density_2017, kingma_glow_2018} and variational inference \parencite{rezende_variational_2016, berg_sylvester_2019}.
In this work, we make use of normalizing flows both as a simple parameterizable invertible function for the $f$-mean, as well as a transformation of the joint posterior into an arbitrary complex distribution in order to improve its ability to capture the underlying factors of multiple modalities.

In practice, flow-based models are typically constructed by implementing the diffeomorphic transformation T (or $T^{-1}$) with a neural network.
Because invertible and differentiable transforms are composable, complex transformations can be built by composing multiple instances of simpler ones: $T=T_F \circ \cdots \circ T_1$.
The density of the transformed posterior $\tilde{q}_{\phi}$ can easily be obtained with the change of variable formula \parencite{bogachev2007measure}:
\begin{equation}
    \tilde{q}_{\phi} = T(q_{\phi}) \where q_{\phi} \sim p_{q_{\phi}}(q_{\phi}) = \mathcal{N}(\mu_{\phi}, \sigma^2_{\phi})
\end{equation}
\begin{equation}
    \label{eq:changeofvariables}
    p_{\tilde{q}_{\phi}}(\tilde{q}_{\phi}) = p_{q_{\phi}}(q_{\phi})|\det J_T(q_{\phi})|^{-1}
\end{equation}

In generative modeling, normalizing flows are used to learn a diffeomorphic mapping $T$ from images to a prior, like Gaussian noise.
Since $T$ is invertible, one can then transform samples from the prior into new images with $T^{-1}$.

For variational inference, normalizing flows are used to transform the posterior into a flexible, arbitrarily complex distribution by transforming it with a normalizing flow.
The transformed posterior can be a much more faithfull approximation of the true underlying distribution than posterior approximations that are limited to one class, like a normal distribution with a diagonal covariance.

In this work, we make use of chained coupling blocks as normalizing flows, implemented by the Framework for Easily Invertible Architectures (FrEIA).
Coupling blocks were first introduced in \citep{dinh_nice_2015} for their Nonlinear Independent Components Estimation (NICE) method.
Chained coupling blocks are constructed such that at every block, the data is split into two halves.
One half is transformed by a simple linear transformation with parameters depending on the other half.
The transformed half is then concatenated with the other, unchanged half.
This process is shown in \cref{fig:coupling block}.
This transformation has a Jacobian which determinant is easily computable since it is triangular.
This gives: $\det Df(x) = \det D\hat{f}(x^B | \Theta (x^A))$.
Affine coupling blocks have been shown to provide good results, especially for image data \citep{dinh_density_2017}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{data/static/coupling_block}
    \caption{Flowchart depicting the workings of a coupling block, taken from the ECCV2020's Tutorial: "Introduction to Normalizing Flows" \citep{coupling_blocks}}
    \label{fig:coupling block}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Importance Weighted Autoencoder} \label{subsec:iwvae}
It has been shown that the tightness of the ELBO in \cref{eq:vaeelbo} can be improved by sampling multiple samples from the posterior at each step \parencite{burda_importance_2016}, which results in the following lower bound:
\begin{equation}
    \label{eq:iwelbo}
    \log p(x) \geq \mathbb{E}_{z_1,\ldots,z_K \sim q_{\phi}(z|x)}\left[ \log \frac{1}{K} \sum ^K _{k=1} \frac{p_{\theta}(x|z_k)p_{\theta}(z)}{q_{\phi}(z_k| x)} \right]\\
    := \mathcal{L}_K
\end{equation}

\Cref{eq:iwelbo} yields useful properties summarized in \parencite{nowozin_debiasing_2018}, namely that one recovers the ELBO for $K=1$, $\mathcal{L}_K$ approaches the true $\log p(x)$ for $K>>1$ ($\lim _{K \rightarrow \inf} \mathcal{L}_K = \log p(x)$) and $\mathcal{L}_1, \ldots, \mathcal{L}_K$ provide stochastic monotonicity ($\mathcal{L}_E = \mathcal{L}_1 \leq \mathcal{L}_2 \leq \ldots \leq \log p(x)$).
The MMVAE from \parencite{shi2019variational} adapts this for multi modal data:
\begin{equation}
    \label{eq:iwelbommvae}
    \mathcal{L}^{MoE}_K(x_{1:M}) =
    \frac{1}{M} \sum _{m=1} ^M \mathbb{E}_{z_m^{1:K} \sim q_{\phi_m}(z|x_m)}\left[ \log \frac{1}{K} \sum ^K _{k=1} \frac{p_{\theta}(x_{1:M}|z_m^k)p_{\theta}(z^k_m)}{q_{\phi}(z_m^k| x_{1:M})} \right]
\end{equation}
which is a valid lower bound of the multi modal log likelihood $\log p(\xset)$.

In our work, we make use of this importance sampling training paradigm to improve the tightness of the ELBO and to approximate the KL-divergence between the posterior and the prior (see \cref{subsubsec:iwMogfM}).

