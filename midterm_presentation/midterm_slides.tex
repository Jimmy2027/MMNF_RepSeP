% pythontex
\input{midterm_presentation/header.tex}

\title[Multi Modal Generative Learning with Normalizing Flows]{Multi Modal Generative Learning with Normalizing Flows}
\subtitle{Midterm Presentation}
\author[Hendrik Klug]{Hendrik Klug}

\AtBeginEnvironment{frame}{\setcounter{footnote}{0}}

\begin{document}

    \include{midterm_presentation/intro}

    \include{midterm_presentation/methods}

    \include{midterm_presentation/results}

    \begin{frame}{Mixture of Flow of Product of Experts VAE}
        Normalizing Flows can also be used to construct \textbf{flexible} and \textbf{arbitrarily complex} latent distributions, by passing the latent distribution through a series of $F$ invertible transformations \citep{rezende_variational_2016, berg_sylvester_2019}:
        \begin{equation*}
            z_F = f_F \circ \ldots \circ f_2 \circ f_1(z_0 \sim q_{\phi}(z|X))
        \end{equation*}
        \begin{equation*}
            \ln q_F(z_F) = \ln q_0 (z_0) - \sum _{f=1} ^{F}\ln \left|  \det \frac{df_f}{dz_{f-1}}\right|
        \end{equation*}
        such that:
        \begin{small}

            \begin{equation*}
                \mathcal{L}_{mofop} := \mathbb{E}_{q_{\phi}(\textbf{z}|\mathbb{X})}[\log (p_{\theta}(\mathbb{X}|\textbf{z}))] - D_{KL}\biggl( \frac{1}{2^M} \sum _{\mathbb{X}_k \in \mathcal{P}(\mathbb{X})} \tilde{q}_{\phi}(\textbf{z}|\mathbb{X}_k)\ ||\ p_{\theta}(\textbf{z})\biggr)
            \end{equation*}

            \begin{equation*}
                D_{KL}(\tilde{q}_{F; \phi_i}(\textbf{z}_{F,i}|X)\ ||\ P_{\theta}(z)) =
                \mathbb{E}_{\tilde{q}_{0; \phi_i}} \left[ \log \tilde{q}_{0; \phi_i}(\textbf{z}_0|X) -\log p_{\theta}(z) \right] - \mathbb{E}_{\tilde{q}_{0; \phi_i}}\left[ \sum_{f=1}^{F} \log \left| \det \left( \frac{d f_f (\textbf{z}_{f-1}; \lambda_f)}{d\textbf{z}_{f-1}} \right) \right| \right]
            \end{equation*}
        \end{small}

    \end{frame}

    \begin{frame}{Mixture of Flow of Product of Experts VAE}
        \begin{figure}
            \centering
            \resizebox{0.9\textwidth}{!}{%
                \py{pytex_printonly(script='midterm_presentation/scripts/mofop_graph.py', data = '')}
            }
        \end{figure}
        \begin{footnotesize}
            \begin{equation*}
                \mathcal{L}_{mofop} := \mathbb{E}_{q_{\phi}(\textbf{z}|\mathbb{X})}[\log (p_{\theta}(\mathbb{X}|\textbf{z}))] - \frac{1}{2^M} \left( \sum_{\mathbb{X}_k \in \mathcal{P}(\mathbb{X})} D_{KL}(q_{0; \phi}(\textbf{z}_0|X)\ ||\ p_{\theta}(\textbf{z})) - \mathbb{E}_{\tilde{q}_{0}}\left[ \sum_{f=1}^{F} \log \left| \det \left( \frac{d f_f (\textbf{z}_{f-1}; \lambda_f)}{d\textbf{z}_{f-1}} \right) \right| \right] \right)
            \end{equation*}
        \end{footnotesize}
    \end{frame}

    \begin{frame}{To Do}
        \begin{itemize}
            \item Continue the work in progress of the current methods
            \item Implement and test Importance Weighted MoPoE
            \item Find an upper bound of the KL-Divergence
            \item Evaluate methods on other datasets (MNIST-SVHN-Text, MIMIC-CXR)
            \vspace{3cm}
            \pause
%            \item Implement and test a method where the joint latent distribution is passed through a normalizing flow
            \item Write Thesis
        \end{itemize}
    \end{frame}



    % Disadvantages: slow
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \begin{frame}
        \centering
        \textbf{\huge{Thank you!}}
    \end{frame}


    \begin{frame}[allowframebreaks]
        \frametitle{References}
        \setbeamertemplate{bibliography item}{}
        \renewcommand*{\bibfont}{\tiny}
        \printbibliography
    \end{frame}

    \include{midterm_presentation/supplementary}

\end{document}
