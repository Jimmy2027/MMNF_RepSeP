@article{sutter_learning_nodate,
    title = {Learning from multiple data types utilizing abstract mean functions},
    pages = {4},
    author = {Sutter, Thonas M},
    langid = {english},
    file = {Sutter - Learning from multiple data types utilizing abstra.pdf:/Users/Hendrik/Zotero/storage/M2B8W74M/Sutter - Learning from multiple data types utilizing abstra.pdf:application/pdf}
}

@article{rezende_variational_2016,
    title = {Variational Inference with Normalizing Flows},
    url = {http://arxiv.org/abs/1505.05770},
    abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
    journaltitle = {{arXiv}:1505.05770 [cs, stat]},
    author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
    urldate = {2021-03-22},
    date = {2016-06-14},
    eprinttype = {arxiv},
    eprint = {1505.05770},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/EFPQNS97/Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf:application/pdf;arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/VSGUU2S3/1505.html:text/html}
}

@article{kingma_improving_2017,
    title = {Improving Variational Inference with Inverse Autoregressive Flow},
    url = {http://arxiv.org/abs/1606.04934},
    abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow ({IAF}), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that {IAF} significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with {IAF}, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
    journaltitle = {{arXiv}:1606.04934 [cs, stat]},
    author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
    urldate = {2021-03-22},
    date = {2017-01-30},
    eprinttype = {arxiv},
    eprint = {1606.04934},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/XL6NVUH7/Kingma et al. - 2017 - Improving Variational Inference with Inverse Autor.pdf:application/pdf;arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/DB4II29Q/1606.html:text/html}
}

@article{kobyzev_normalizing_2020,
    title = {Normalizing Flows: An Introduction and Review of Current Methods},
    issn = {1939-3539},
    doi = {10.1109/TPAMI.2020.2992934},
    shorttitle = {Normalizing Flows},
    abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
    pages = {1--1},
    journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
    author = {Kobyzev, I. and Prince, S. and Brubaker, M.},
    date = {2020},
    note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
    keywords = {Computational modeling, Context modeling, Density estimation, Estimation, Generative models, Invertible neural networks, Jacobian matrices, Mathematical model, Normalizing flows, Random variables, Training, Variational inference},
    file = {IEEE Xplore Full Text PDF:/Users/Hendrik/Zotero/storage/7IGXE2K7/Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf:application/pdf}
}

@article{papamakarios_normalizing_2019,
    title = {Normalizing Flows for Probabilistic Modeling and Inference},
    url = {http://arxiv.org/abs/1912.02762},
    abstract = {Normalizing ﬂows provide a general mechanism for deﬁning expressive probability distributions, only requiring the speciﬁcation of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing ﬂows, ranging from improving their expressive power to expanding their application. We believe the ﬁeld has now matured and is in need of a uniﬁed perspective. In this review, we attempt to provide such a perspective by describing ﬂows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of ﬂow design, and discuss foundational topics such as expressive power and computational trade-oﬀs. We also broaden the conceptual framing of ﬂows by relating them to more general probability transformations. Lastly, we summarize the use of ﬂows for tasks such as generative modeling, approximate inference, and supervised learning.},
    journaltitle = {{arXiv}:1912.02762 [cs, stat]},
    author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
    urldate = {2021-03-24},
    date = {2019-12-05},
    langid = {english},
    eprinttype = {arxiv},
    eprint = {1912.02762},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {Papamakarios et al. - 2019 - Normalizing Flows for Probabilistic Modeling and I.pdf:/Users/Hendrik/Zotero/storage/76PKLB7E/Papamakarios et al. - 2019 - Normalizing Flows for Probabilistic Modeling and I.pdf:application/pdf}
}

@article{baltrusaitis_multimodal_2019,
    title = {Multimodal Machine Learning: A Survey and Taxonomy},
    volume = {41},
    issn = {1939-3539},
    doi = {10.1109/TPAMI.2018.2798607},
    shorttitle = {Multimodal Machine Learning},
    abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
    pages = {423--443},
    number = {2},
    journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
    author = {Baltrušaitis, T. and Ahuja, C. and Morency, L.},
    date = {2019-02},
    note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
    keywords = {Hidden Markov models, introductory, machine learning, Media, Multimedia communication, Multimodal, Speech, Speech recognition, Streaming media, survey, Visualization},
    file = {IEEE Xplore Abstract Record:/Users/Hendrik/Zotero/storage/GRLQHZ3N/8269806.html:text/html;IEEE Xplore Full Text PDF:/Users/Hendrik/Zotero/storage/9X742YZK/Baltrušaitis et al. - 2019 - Multimodal Machine Learning A Survey and Taxonomy.pdf:application/pdf}
}

@inproceedings{karpathy_deep_2015,
    title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
    url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.html},
    eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
    pages = {3128--3137},
    author = {Karpathy, Andrej and Fei-Fei, Li},
    urldate = {2021-03-30},
    date = {2015},
    file = {Full Text PDF:/Users/Hendrik/Zotero/storage/B48CM24P/Karpathy and Fei-Fei - 2015 - Deep Visual-Semantic Alignments for Generating Ima.pdf:application/pdf;Snapshot:/Users/Hendrik/Zotero/storage/JZ7JVFPQ/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.html:text/html}
}

@article{kingma_auto-encoding_2014,
    title = {Auto-Encoding Variational Bayes},
    url = {http://arxiv.org/abs/1312.6114},
    abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
    journaltitle = {{arXiv}:1312.6114 [cs, stat]},
    author = {Kingma, Diederik P. and Welling, Max},
    urldate = {2021-03-30},
    date = {2014-05-01},
    eprinttype = {arxiv},
    eprint = {1312.6114},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/YN7CSDUJ/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/QX9BK2YR/1312.html:text/html}
}

@book{niculescu_convex_2018,
    location = {Cham},
    title = {Convex Functions and Their Applications},
    isbn = {978-3-319-78336-9 978-3-319-78337-6},
    url = {http://link.springer.com/10.1007/978-3-319-78337-6},
    series = {{CMS} Books in Mathematics},
    publisher = {Springer International Publishing},
    author = {Niculescu, Constantin P. and Persson, Lars-Erik},
    urldate = {2021-03-30},
    date = {2018},
    langid = {english},
    doi = {10.1007/978-3-319-78337-6},
    file = {Niculescu and Persson - 2018 - Convex Functions and Their Applications.pdf:/Users/Hendrik/Zotero/storage/PWHQL3RY/Niculescu and Persson - 2018 - Convex Functions and Their Applications.pdf:application/pdf}
}


@article{shi_variational_2019,
    title = {Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models},
    url = {http://arxiv.org/abs/1911.03393},
    abstract = {Learning generative models that span multiple data modalities, such as vision and language, is often motivated by the desire to learn more useful, generalisable representations that faithfully capture common underlying factors between the modalities. In this work, we characterise successful learning of such models as the fulfillment of four criteria: i) implicit latent decomposition into shared and private subspaces, ii) coherent joint generation over all modalities, iii) coherent cross-generation across individual modalities, and iv) improved model learning for individual modalities through multi-modal integration. Here, we propose a mixture-of-experts multimodal variational autoencoder ({MMVAE}) to learn generative models on different sets of modalities, including a challenging image-language dataset, and demonstrate its ability to satisfy all four criteria, both qualitatively and quantitatively.},
    journaltitle = {{arXiv}:1911.03393 [cs, stat]},
    author = {Shi, Yuge and Siddharth, N. and Paige, Brooks and Torr, Philip H. S.},
    urldate = {2021-03-30},
    date = {2019-11-08},
    eprinttype = {arxiv},
    eprint = {1911.03393},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/RCT86RBU/Shi et al. - 2019 - Variational Mixture-of-Experts Autoencoders for Mu.pdf:application/pdf;arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/LB9ZB8L7/1911.html:text/html}
}

@article{sutter_multimodal_2020,
    title = {Multimodal Generative Learning Utilizing Jensen-Shannon-Divergence},
    url = {http://arxiv.org/abs/2006.08242},
    abstract = {Learning from different data types is a long-standing goal in machine learning research, as multiple information sources co-occur when describing natural phenomena. However, existing generative models that approximate a multimodal {ELBO} rely on difficult or inefficient training schemes to learn a joint distribution and the dependencies between modalities. In this work, we propose a novel, efficient objective function that utilizes the Jensen-Shannon divergence for multiple distributions. It simultaneously approximates the unimodal and joint multimodal posteriors directly via a dynamic prior. In addition, we theoretically prove that the new multimodal {JS}-divergence ({mmJSD}) objective optimizes an {ELBO}. In extensive experiments, we demonstrate the advantage of the proposed {mmJSD} model compared to previous work in unsupervised, generative learning tasks.},
    journaltitle = {{arXiv}:2006.08242 [cs, stat]},
    author = {Sutter, Thomas M. and Daunhawer, Imant and Vogt, Julia E.},
    urldate = {2021-03-30},
    date = {2020-11-02},
    eprinttype = {arxiv},
    eprint = {2006.08242},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/2SGSPF93/Sutter et al. - 2020 - Multimodal Generative Learning Utilizing Jensen-Sh.pdf:application/pdf;arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/4VI64PCH/2006.html:text/html}
}

@article{suzuki_joint_2016,
    title = {Joint Multimodal Learning with Deep Generative Models},
    url = {http://arxiv.org/abs/1611.01891},
    abstract = {We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders ({VAEs}). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder ({JMVAE}), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, {JMVAE}-kl, that is trained by reducing the divergence between {JMVAE}'s encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional {VAEs}. We further demonstrate that {JMVAE} can generate multiple modalities bi-directionally.},
    journaltitle = {{arXiv}:1611.01891 [cs, stat]},
    author = {Suzuki, Masahiro and Nakayama, Kotaro and Matsuo, Yutaka},
    urldate = {2021-03-30},
    date = {2016-11-06},
    eprinttype = {arxiv},
    eprint = {1611.01891},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/UZXUCHRL/Suzuki et al. - 2016 - Joint Multimodal Learning with Deep Generative Mod.pdf:application/pdf;arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/2K9XPIJP/1611.html:text/html}
}

@inproceedings{tsai_learning_2018,
    title = {Learning Factorized Multimodal Representations},
    url = {https://openreview.net/forum?id=rygqqsA9KX},
    abstract = {We propose a model to learn factorized multimodal representations that are discriminative, generative, and interpretable.},
    eventtitle = {International Conference on Learning Representations},
    author = {Tsai, Yao-Hung Hubert and Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
    urldate = {2021-03-30},
    date = {2018-09-27},
    langid = {english},
    file = {Full Text PDF:/Users/Hendrik/Zotero/storage/6A6KSPN4/Tsai et al. - 2018 - Learning Factorized Multimodal Representations.pdf:application/pdf;Snapshot:/Users/Hendrik/Zotero/storage/GDQUU3I9/forum.html:text/html}
}

@article{vedantam_generative_2018,
    title = {Generative Models of Visually Grounded Imagination},
    url = {http://arxiv.org/abs/1705.10762},
    abstract = {It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially specified (abstract) concepts in a principled and efficient way. We also propose a set of easy-to-compute evaluation metrics that capture our intuitive notions of what it means to have good visual imagination, namely correctness, coverage, and compositionality (the 3 C's). Finally, we perform a detailed comparison of our method with two existing joint image-attribute {VAE} methods (the {JMVAE} method of Suzuki et.al. and the {BiVCCA} method of Wang et.al.) by applying them to two datasets: the {MNIST}-with-attributes dataset (which we introduce here), and the {CelebA} dataset.},
    journaltitle = {{arXiv}:1705.10762 [cs, stat]},
    author = {Vedantam, Ramakrishna and Fischer, Ian and Huang, Jonathan and Murphy, Kevin},
    urldate = {2021-03-30},
    date = {2018-11-09},
    eprinttype = {arxiv},
    eprint = {1705.10762},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/KB67TLNP/Vedantam et al. - 2018 - Generative Models of Visually Grounded Imagination.pdf:application/pdf;arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/39Y5UK8T/1705.html:text/html}
}

@article{poe,
    title = {Multimodal Generative Models for Scalable Weakly-Supervised Learning},
    url = {http://arxiv.org/abs/1802.05335},
    abstract = {Multiple modalities often co-occur when describing natural phenomena. Learning a joint representation of these modalities should yield deeper and more useful representations. Previous generative approaches to multi-modal input either do not learn a joint distribution or require additional computation to handle missing data. Here, we introduce a multimodal variational autoencoder ({MVAE}) that uses a product-of-experts inference network and a sub-sampled training paradigm to solve the multi-modal inference problem. Notably, our model shares parameters to efficiently learn under any combination of missing modalities. We apply the {MVAE} on four datasets and match state-of-the-art performance using many fewer parameters. In addition, we show that the {MVAE} is directly applicable to weakly-supervised learning, and is robust to incomplete supervision. We then consider two case studies, one of learning image transformations---edge detection, colorization, segmentation---as a set of modalities, followed by one of machine translation between two languages. We find appealing results across this range of tasks.},
    journaltitle = {{arXiv}:1802.05335 [cs, stat]},
    author = {Wu, Mike and Goodman, Noah},
    urldate = {2021-03-30},
    date = {2018-11-12},
    eprinttype = {arxiv},
    eprint = {1802.05335},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@article{johnson_mimic-cxr-jpg_2019,
    title = {{MIMIC}-{CXR}-{JPG}, a large publicly available database of labeled chest radiographs},
    url = {http://arxiv.org/abs/1901.07042},
    abstract = {Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's thorax, but requiring specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. However, a key challenge in the development of these techniques is the lack of sufficient data. Here we describe {MIMIC}-{CXR}-{JPG} v2.0.0, a large dataset of 377,110 chest x-rays associated with 227,827 imaging studies sourced from the Beth Israel Deaconess Medical Center between 2011 - 2016. Images are provided with 14 labels derived from two natural language processing tools applied to the corresponding free-text radiology reports. {MIMIC}-{CXR}-{JPG} is derived entirely from the {MIMIC}-{CXR} database, and aims to provide a convenient processed version of {MIMIC}-{CXR}, as well as to provide a standard reference for data splits and image labels. All images have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in medical computer vision.},
    journaltitle = {{arXiv}:1901.07042 [cs, eess]},
    author = {Johnson, Alistair E. W. and Pollard, Tom J. and Greenbaum, Nathaniel R. and Lungren, Matthew P. and Deng, Chih-ying and Peng, Yifan and Lu, Zhiyong and Mark, Roger G. and Berkowitz, Seth J. and Horng, Steven},
    urldate = {2021-03-31},
    date = {2019-11-14},
    eprinttype = {arxiv},
    eprint = {1901.07042},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/J6MBHIJ4/Johnson et al. - 2019 - MIMIC-CXR-JPG, a large publicly available database.pdf:application/pdf;arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/TCH8K5K9/1901.html:text/html}
}

@book{bogachev2007measure,
    title = {Measure theory},
    author = {Bogachev, Vladimir I},
    volume = {1},
    year = {2007},
    publisher = {Springer Science \& Business Media}
}


@inproceedings{huang_neural_2018,
    title = {Neural Autoregressive Flows},
    url = {http://proceedings.mlr.press/v80/huang18d.html},
    abstract = {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows ({MAF}) (Papamakarios et al., 20...},
    eventtitle = {International Conference on Machine Learning},
    pages = {2078--2087},
    booktitle = {International Conference on Machine Learning},
    publisher = {{PMLR}},
    author = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
    urldate = {2021-04-01},
    date = {2018-07-03},
    langid = {english},
    note = {{ISSN}: 2640-3498},
    file = {Full Text PDF:/Users/Hendrik/Zotero/storage/XRY3BRP2/Huang et al. - 2018 - Neural Autoregressive Flows.pdf:application/pdf;Snapshot:/Users/Hendrik/Zotero/storage/2XQCARBD/huang18d.html:text/html}
}


@article{hochreiter_long_1997,
    title = {Long Short-Term Memory},
    volume = {9},
    issn = {0899-7667},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    doi = {10.1162/neco.1997.9.8.1735},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    pages = {1735--1780},
    number = {8},
    journaltitle = {Neural Computation},
    shortjournal = {Neural Computation},
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    urldate = {2021-04-01},
    date = {1997-11-15},
    file = {Full Text PDF:/Users/Hendrik/Zotero/storage/HCEHAKYR/Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf:application/pdf;Snapshot:/Users/Hendrik/Zotero/storage/6USAAPTG/Long-Short-Term-Memory.html:text/html}
}


@article{dinh_nice_2015,
    title = {{NICE}: Non-linear Independent Components Estimation},
    url = {http://arxiv.org/abs/1410.8516},
    shorttitle = {{NICE}},
    abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation ({NICE}). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
    journaltitle = {{arXiv}:1410.8516 [cs]},
    author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
    urldate = {2021-04-01},
    date = {2015-04-10},
    eprinttype = {arxiv},
    eprint = {1410.8516},
    keywords = {Computer Science - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/STEZXYWX/Dinh et al. - 2015 - NICE Non-linear Independent Components Estimation.pdf:application/pdf;arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/A6T37WCB/1410.html:text/html}
}


@article{kingma_glow_2018,
    title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
    url = {http://arxiv.org/abs/1807.03039},
    shorttitle = {Glow},
    abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
    journaltitle = {{arXiv}:1807.03039 [cs, stat]},
    author = {Kingma, Diederik P. and Dhariwal, Prafulla},
    urldate = {2021-04-01},
    date = {2018-07-10},
    eprinttype = {arxiv},
    eprint = {1807.03039},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}


@inproceedings{prenger_waveglow_2019,
    title = {Waveglow: A Flow-based Generative Network for Speech Synthesis},
    doi = {10.1109/ICASSP.2019.8683143},
    shorttitle = {Waveglow},
    abstract = {In this paper we propose {WaveGlow}: a flow-based network capable of generating high quality speech from mel-spectrograms. {WaveGlow} combines insights from Glow [1] and {WaveNet} [2] in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. {WaveGlow} is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our {PyTorch} implementation produces audio samples at a rate of more than 500 {kHz} on an {NVIDIA} V100 {GPU}. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available {WaveNet} implementation. All code will be made publicly available online [3].},
    eventtitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
    pages = {3617--3621},
    booktitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
    author = {Prenger, R. and Valle, R. and Catanzaro, B.},
    date = {2019-05},
    note = {{ISSN}: 2379-190X},
    keywords = {Audio Synthesis, Deep Learning, Generative models, Text-to-speech},
    file = {IEEE Xplore Abstract Record:/Users/Hendrik/Zotero/storage/HDWS5R4T/8683143.html:text/html;IEEE Xplore Full Text PDF:/Users/Hendrik/Zotero/storage/3SGZ4BAC/Prenger et al. - 2019 - Waveglow A Flow-based Generative Network for Spee.pdf:application/pdf}
}


@inproceedings{ho_flow_2019,
    title = {Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design},
    url = {http://proceedings.mlr.press/v97/ho19a.html},
    shorttitle = {Flow++},
    abstract = {Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. Despite their computational efficiency, flow-based models generally have much worse density ...},
    eventtitle = {International Conference on Machine Learning},
    pages = {2722--2730},
    booktitle = {International Conference on Machine Learning},
    publisher = {{PMLR}},
    author = {Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
    urldate = {2021-04-01},
    date = {2019-05-24},
    langid = {english},
    note = {{ISSN}: 2640-3498},
    file = {Full Text PDF:/Users/Hendrik/Zotero/storage/LQ3MUA3C/Ho et al. - 2019 - Flow++ Improving Flow-Based Generative Models wit.pdf:application/pdf;Snapshot:/Users/Hendrik/Zotero/storage/X94P6YD7/ho19a.html:text/html}
}


@inproceedings{behrmann_invertible_2019,
    title = {Invertible Residual Networks},
    url = {http://proceedings.mlr.press/v97/behrmann19a.html},
    abstract = {We show that standard {ResNet} architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requ...},
    eventtitle = {International Conference on Machine Learning},
    pages = {573--582},
    booktitle = {International Conference on Machine Learning},
    publisher = {{PMLR}},
    author = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, Joern-Henrik},
    urldate = {2021-04-02},
    date = {2019-05-24},
    langid = {english},
    note = {{ISSN}: 2640-3498},
    file = {Full Text PDF:/Users/Hendrik/Zotero/storage/4MI5NR64/Behrmann et al. - 2019 - Invertible Residual Networks.pdf:application/pdf;Snapshot:/Users/Hendrik/Zotero/storage/CNUKZKKM/behrmann19a.html:text/html}
}


@article{chen_residual_2020,
    title = {Residual Flows for Invertible Generative Modeling},
    url = {http://arxiv.org/abs/1906.02735},
    abstract = {Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estimate of the log density using a "Russian roulette" estimator, and reduce the memory required during training by using an alternative infinite series for the gradient. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid derivative saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.},
    journaltitle = {{arXiv}:1906.02735 [cs, stat]},
    author = {Chen, Ricky T. Q. and Behrmann, Jens and Duvenaud, David and Jacobsen, Jörn-Henrik},
    urldate = {2021-04-02},
    date = {2020-07-23},
    eprinttype = {arxiv},
    eprint = {1906.02735},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/UDQ2Q5YG/Chen et al. - 2020 - Residual Flows for Invertible Generative Modeling.pdf:application/pdf}
}


@article{berg_sylvester_2019,
    title = {Sylvester Normalizing Flows for Variational Inference},
    url = {http://arxiv.org/abs/1803.05649},
    abstract = {Variational inference relies on flexible approximate posterior distributions. Normalizing flows provide a general recipe to construct flexible variational posteriors. We introduce Sylvester normalizing flows, which can be seen as a generalization of planar flows. Sylvester normalizing flows remove the well-known single-unit bottleneck from planar flows, making a single transformation much more flexible. We compare the performance of Sylvester normalizing flows against planar flows and inverse autoregressive flows and demonstrate that they compare favorably on several datasets.},
    journaltitle = {{arXiv}:1803.05649 [cs, stat]},
    author = {Berg, Rianne van den and Hasenclever, Leonard and Tomczak, Jakub M. and Welling, Max},
    urldate = {2021-04-02},
    date = {2019-02-20},
    eprinttype = {arxiv},
    eprint = {1803.05649},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/8QMQLHTS/Berg et al. - 2019 - Sylvester Normalizing Flows for Variational Infere.pdf:application/pdf;arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/9WA6H4TB/1803.html:text/html}
}


@inproceedings{sutter_generalized_2020,
    title = {Generalized Multimodal {ELBO}},
    url = {https://openreview.net/forum?id=5Y21V0RDBV},
    abstract = {Multiple data types naturally co-occur when describing real-world phenomena and learning from them is a long-standing goal in machine learning research. However, existing self-supervised generative...},
    eventtitle = {International Conference on Learning Representations},
    author = {Sutter, Thomas Marco and Daunhawer, Imant and Vogt, Julia E.},
    urldate = {2021-04-02},
    date = {2020-09-28},
    langid = {english},
    file = {Full Text PDF:/Users/Hendrik/Zotero/storage/I754DQ4N/Sutter et al. - 2020 - Generalized Multimodal ELBO.pdf:application/pdf;Snapshot:/Users/Hendrik/Zotero/storage/93R3JL2U/forum.html:text/html}
}


@inproceedings{liu_deep_2015,
    title = {Deep Learning Face Attributes in the Wild},
    url = {https://openaccess.thecvf.com/content_iccv_2015/html/Liu_Deep_Learning_Face_ICCV_2015_paper.html},
    eventtitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
    pages = {3730--3738},
    author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
    urldate = {2021-04-08},
    date = {2015},
    file = {Full Text PDF:/Users/Hendrik/Zotero/storage/3EKADQG2/Liu et al. - 2015 - Deep Learning Face Attributes in the Wild.pdf:application/pdf;Snapshot:/Users/Hendrik/Zotero/storage/J2XUSGJI/Liu_Deep_Learning_Face_ICCV_2015_paper.html:text/html}
}


@article{oord_conditional_2016,
    title = {Conditional Image Generation with {PixelCNN} Decoders},
    url = {http://arxiv.org/abs/1606.05328},
    abstract = {This work explores conditional image generation with a new image density model based on the {PixelCNN} architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the {ImageNet} database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional {PixelCNN} can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of {PixelCNN} to match the state-of-the-art performance of {PixelRNN} on {ImageNet}, with greatly reduced computational cost.},
    journaltitle = {{arXiv}:1606.05328 [cs]},
    author = {Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
    urldate = {2021-04-02},
    date = {2016-06-18},
    eprinttype = {arxiv},
    eprint = {1606.05328},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
    file = {arXiv Fulltext PDF:/Users/Hendrik/Zotero/storage/CVFIJQ4E/Oord et al. - 2016 - Conditional Image Generation with PixelCNN Decoder.pdf:application/pdf;arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/Q6DVZKM7/1606.html:text/html}
}
@inproceedings{imagenet_cvpr09,
    AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
    TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
    BOOKTITLE = {CVPR09},
    YEAR = {2009},
    BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib" }

@article{huang2018densely,
    title = {Densely connected convolutional networks. arXiv 2016},
    author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q and van der Maaten, Laurens},
    journal = {arXiv preprint arXiv:1608.06993},
    volume = {1608},
    year = {2018}
}

@misc{vahdat2021nvae,
    title = {NVAE: A Deep Hierarchical Variational Autoencoder},
    author = {Arash Vahdat and Jan Kautz},
    year = {2021},
    eprint = {2007.03898},
    archivePrefix = {arXiv},
    primaryClass = {stat.ML}
}

@article{zhao2017towards,
    title = {Towards deeper understanding of variational autoencoding models},
    author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
    journal = {arXiv preprint arXiv:1702.08658},
    year = {2017}
}

@misc{oord2018neural,
    title = {Neural Discrete Representation Learning},
    author = {Aaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu},
    year = {2018},
    eprint = {1711.00937},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@inproceedings{liu2015faceattributes,
    title = {Deep Learning Face Attributes in the Wild},
    author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
    booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
    month = {December},
    year = {2015}
}

@article{scikit,
    title = {Scikit-learn: Machine learning in Python},
    author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
    journal = {the Journal of machine Learning research},
    volume = {12},
    pages = {2825--2830},
    year = {2011},
    publisher = {JMLR. org}
}


@article{lecun-mnisthandwrittendigit-2010,
    added-at = {2010-06-28T21:16:30.000+0200},
    author = {LeCun, Yann and Cortes, Corinna},
    biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
    groups = {public},
    howpublished = {http://yann.lecun.com/exdb/mnist/},
    interhash = {21b9d0558bd66279df9452562df6e6f3},
    intrahash = {935bad99fa1f65e03c25b315aa3c1032},
    keywords = {MSc _checked character_recognition mnist network neural},
    lastchecked = {2016-01-14 14:24:11},
    timestamp = {2016-07-12T19:25:30.000+0200},
    title = {{MNIST} handwritten digit database},
    url = {http://yann.lecun.com/exdb/mnist/},
    username = {mhwombat},
    year = 2010
}

@article{adam,
    title = {A method for stochastic optimization. arXiv 2014},
    author = {Kingma, Diederik P and Ba, J Adam},
    journal = {arXiv preprint arXiv:1412.6980},
    volume = {434},
    year = {2019}
}

@misc{kingma2014autoencoding,
    title = {Auto-Encoding Variational Bayes},
    author = {Diederik P Kingma and Max Welling},
    year = {2014},
    eprint = {1312.6114},
    archivePrefix = {arXiv},
    primaryClass = {stat.ML}
}

@misc{doersch2016tutorial,
    title = {Tutorial on Variational Autoencoders},
    author = {Carl Doersch},
    year = {2016},
    eprint = {1606.05908},
    archivePrefix = {arXiv},
    primaryClass = {stat.ML}
}

@inproceedings{wu2018multimodal,
    title = {Multimodal generative models for scalable weakly-supervised learning},
    author = {Wu, Mike and Goodman, Noah},
    booktitle = {Advances in Neural Information Processing Systems},
    pages = {5575--5585},
    year = {2018}
}

@article{netzer2011reading,
    title = {Reading digits in natural images with unsupervised feature learning},
    author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
    year = {2011}
}
@inproceedings{shi2019variational,
    title = {Variational mixture-of-experts autoencoders for multi-modal deep generative models},
    author = {Shi, Yuge and Siddharth, Narayanaswamy and Paige, Brooks and Torr, Philip},
    booktitle = {Advances in Neural Information Processing Systems},
    pages = {15718--15729},
    year = {2019}
}

@inproceedings{liu2015deep,
    title = {Deep learning face attributes in the wild},
    author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
    booktitle = {Proceedings of the IEEE international conference on computer vision},
    pages = {3730--3738},
    year = {2015}
}

@article{thomas_multimodal,
    title = {Multimodal Generative Learning Utilizing Jensen-Shannon-Divergence},
    author = {Sutter, Thomas M and Daunhawer, Imant and Vogt, Julia E},
    journal = {arXiv preprint arXiv:2006.08242},
    year = {2020}
}

@article{bowman2015generating,
    title = {Generating sentences from a continuous space},
    author = {Bowman, Samuel R and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
    journal = {arXiv preprint arXiv:1511.06349},
    year = {2015}
}

@article{gulrajani2016pixelvae,
    title = {Pixelvae: A latent variable model for natural images},
    author = {Gulrajani, Ishaan and Kumar, Kundan and Ahmed, Faruk and Taiga, Adrien Ali and Visin, Francesco and Vazquez, David and Courville, Aaron},
    journal = {arXiv preprint arXiv:1611.05013},
    year = {2016}
}

@article{kingma2015variational,
    title = {Variational dropout and the local reparameterization trick},
    author = {Kingma, Diederik P and Salimans, Tim and Welling, Max},
    journal = {arXiv preprint arXiv:1506.02557},
    year = {2015}
}

@inproceedings{das2017visual,
    title = {Visual dialog},
    author = {Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, Jos{\'e} MF and Parikh, Devi and Batra, Dhruv},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    pages = {326--335},
    year = {2017}
}


@inproceedings{antol2015vqa,
    title = {Vqa: Visual question answering},
    author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
    booktitle = {Proceedings of the IEEE international conference on computer vision},
    pages = {2425--2433},
    year = {2015}
}

@article{dosovitskiy2016generating,
    title = {Generating images with perceptual similarity metrics based on deep networks},
    author = {Dosovitskiy, Alexey and Brox, Thomas},
    journal = {arXiv preprint arXiv:1602.02644},
    year = {2016}
}

@inproceedings{kovaleva2020towards,
    title = {Towards Visual Dialog for Radiology},
    author = {Kovaleva, Olga and Shivade, Chaitanya and Kashyap, Satyananda and Kanjaria, Karina and Wu, Joy and Ballah, Deddeh and Coy, Adam and Karargyris, Alexandros and Guo, Yufan and Beymer, David Beymer and others},
    booktitle = {Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing},
    pages = {60--69},
    year = {2020}
}

@inproceedings{he2016deep,
    title = {Deep residual learning for image recognition},
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages = {770--778},
    year = {2016}
}


@software{mimic_repsep,
    title = {Jimmy2027/Mimic\_RepSep},
    rights = {{GPL}-3.0 License ,                 {GPL}-3.0 License},
    url = {https://github.com/Jimmy2027/Mimic_RepSep},
    abstract = {Contribute to Jimmy2027/Mimic\_RepSep development by creating an account on {GitHub}.},
    author = {Hendrik{\textbackslash}\_Klug},
    urldate = {2021-02-16},
    date = {2021-02-15},
    note = {original-date: 2021-01-16T14:03:58Z}
}

@article{ioanas2018reproducible,
    title = {Reproducible self-publishing for Python-based research},
    author = {Ioanas, Horea-Ioan and Rudin, Markus},
    journal = {EuroSciPy, August},
    year = {2018}
}


@inproceedings{
thomas_gener-ELBO,
    title = {Generalized Multimodal {ELBO}},
    author = {Thomas Marco Sutter and Imant Daunhawer and Julia E Vogt},
    booktitle = {International Conference on Learning Representations},
    year = {2021},
}

@article{rajpurkar2017chexnet,
    title = {Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning},
    author = {Rajpurkar, Pranav and Irvin, Jeremy and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis and Shpanskaya, Katie and others},
    journal = {arXiv preprint arXiv:1711.05225},
    year = {2017}
}

@article{johnson2019mimic,
    title = {MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs},
    author = {Johnson, Alistair EW and Pollard, Tom J and Greenbaum, Nathaniel R and Lungren, Matthew P and Deng, Chih-ying and Peng, Yifan and Lu, Zhiyong and Mark, Roger G and Berkowitz, Seth J and Horng, Steven},
    journal = {arXiv preprint arXiv:1901.07042},
    year = {2019}
}


@article{dinh_density_2017,
    title = {Density estimation using Real {NVP}},
    url = {http://arxiv.org/abs/1605.08803},
    abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real {NVP}) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
    journaltitle = {{arXiv}:1605.08803 [cs, stat]},
    author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
    urldate = {2021-07-29},
    date = {2017-02-27},
    eprinttype = {arxiv},
    eprint = {1605.08803},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}




@article{burda_importance_2016,
    title = {Importance Weighted Autoencoders},
    url = {http://arxiv.org/abs/1509.00519},
    abstract = {The variational autoencoder ({VAE}; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the {VAE} objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder ({IWAE}), a generative model with the same architecture as the {VAE}, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the {IWAE}, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the {VAE} modeling assumptions. We show empirically that {IWAEs} learn richer latent space representations than {VAEs}, leading to improved test log-likelihood on density estimation benchmarks.},
    journaltitle = {{arXiv}:1509.00519 [cs, stat]},
    author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
    urldate = {2021-07-29},
    date = {2016-11-07},
    eprinttype = {arxiv},
    eprint = {1509.00519},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, \_tablet, to\_read},
    file = {arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/NGP9N8LJ/1509.html:text/html;Burda et al_2016_Importance Weighted Autoencoders.pdf:/Users/Hendrik/zotfile/Burda et al_2016_Importance Weighted Autoencoders.pdf:application/pdf}
}


@inproceedings{nowozin_debiasing_2018,
    title = {Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference},
    url = {https://www.microsoft.com/en-us/research/publication/debiasing-evidence-approximations-importance-weighted-autoencoders-jackknife-variational-inference/},
    shorttitle = {Debiasing Evidence Approximations},
    abstract = {The importance-weighted autoencoder ({IWAE}) approach of Burda et al. defines a sequence of increasingly tighter bounds on the marginal likelihood of latent variable models. Recently, Cremer et al. reinterpreted the {IWAE} bounds as ordinary variational evidence lower bounds ({ELBO}) applied to increasingly accurate variational distributions. In this work, we provide yet another perspective on the […]},
    eventtitle = {{ICLR} 2018 Conference},
    author = {Nowozin, Sebastian},
    urldate = {2021-07-30},
    date = {2018-02-15},
    langid = {american},
    keywords = {\_tablet},
    file = {Nowozin_2018_Debiasing Evidence Approximations.pdf:/Users/Hendrik/zotfile/Nowozin_2018_Debiasing Evidence Approximations.pdf:application/pdf;Snapshot:/Users/Hendrik/Zotero/storage/2P4TKEGT/debiasing-evidence-approximations-importance-weighted-autoencoders-jackknife-variational-infere.html:text/html}
}

@inproceedings{precision_recall_distributions,
    title = {{Assessing Generative Models via Precision and Recall}},
    author = {Sajjadi, Mehdi~S.~M. and Bachem, Olivier and Lu{\v c}i{\'c}, Mario and Bousquet, Olivier and Gelly, Sylvain},
    booktitle = {{Advances in Neural Information Processing Systems (NeurIPS)}},
    year = {2018} }


@article{akiba_optuna_2019,
    title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
    url = {http://arxiv.org/abs/1907.10902},
    shorttitle = {Optuna},
    abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run {API} that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the {MIT} license (https://github.com/pfnet/optuna/).},
    journaltitle = {{arXiv}:1907.10902 [cs, stat]},
    author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
    urldate = {2021-08-08},
    date = {2019-07-25},
    eprinttype = {arxiv},
    eprint = {1907.10902},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}



@inproceedings{rezende_stochastic_2014,
    title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
    url = {http://proceedings.mlr.press/v32/rezende14.html},
    abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference an...},
    eventtitle = {International Conference on Machine Learning},
    pages = {1278--1286},
    booktitle = {International Conference on Machine Learning},
    publisher = {{PMLR}},
    author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
    urldate = {2021-03-30},
    date = {2014-06-18},
    langid = {english},
    note = {{ISSN}: 1938-7228},

}


@article{beta_vae,
    title = {beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational Framework},
    url = {https://openreview.net/forum?id=Sy2fzU9gl},
    shorttitle = {beta-{VAE}},
    abstract = {We introduce beta-{VAE}, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.},
    author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
    urldate = {2021-05-21},
    date = {2016-11-04},
    langid = {english},
}


@article{heusel_gans_2017,
	title = {{GANs} Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
	url = {http://arxiv.org/abs/1706.08500},
	abstract = {Generative Adversarial Networks ({GANs}) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of {GAN} training has still not been proved. We propose a two time-scale update rule ({TTUR}) for training {GANs} with stochastic gradient descent on arbitrary {GAN} loss functions. {TTUR} has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the {TTUR} converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of {GANs} at image generation, we introduce the "Fr{\textbackslash}'echet Inception Distance" ({FID}) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, {TTUR} improves learning for {DCGANs} and Improved Wasserstein {GANs} ({WGAN}-{GP}) outperforming conventional {GAN} training on {CelebA}, {CIFAR}-10, {SVHN}, {LSUN} Bedrooms, and the One Billion Word Benchmark.},
	journaltitle = {{arXiv}:1706.08500 [cs, stat]},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	urldate = {2021-10-09},
	date = {2017-06-26},
	eprinttype = {arxiv},
	eprint = {1706.08500},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}


@article{chen_isolating_2019,
    title = {Isolating Sources of Disentanglement in Variational Autoencoders},
    url = {http://arxiv.org/abs/1802.04942},
    abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate our \${\textbackslash}beta\$-{TCVAE} (Total Correlation Variational Autoencoder), a refinement of the state-of-the-art \${\textbackslash}beta\$-{VAE} objective for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap ({MIG}). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the latent variables model is trained using our framework.},
    journaltitle = {{arXiv}:1802.04942 [cs, stat]},
    author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
    urldate = {2021-09-11},
    date = {2019-04-23},
    eprinttype = {arxiv},
    eprint = {1802.04942},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, disentanglement, Statistics - Machine Learning},
    file = {arXiv.org Snapshot:/Users/Hendrik/Zotero/storage/ZBXAG9JF/1802.html:text/html;Chen et al_2019_Isolating Sources of Disentanglement in Variational Autoencoders.pdf:/Users/Hendrik/zotfile/Chen et al_2019_Isolating Sources of Disentanglement in Variational Autoencoders.pdf:application/pdf}
}


@article{locatello_challenging_2019,
    title = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
    url = {http://arxiv.org/abs/1811.12359},
    abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
    journaltitle = {{arXiv}:1811.12359 [cs, stat]},
    author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Rätsch, Gunnar and Gelly, Sylvain and Schölkopf, Bernhard and Bachem, Olivier},
    urldate = {2021-09-11},
    date = {2019-06-18},
    eprinttype = {arxiv},
    eprint = {1811.12359},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, disentanglement}
}


@article{hinton_training_2002,
    title = {Training products of experts by minimizing contrastive divergence},
    volume = {14},
    issn = {0899-7667},
    doi = {10.1162/089976602760128018},
    abstract = {It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual "expert" models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts ({PoE}) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a {PoE} by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a {PoE} can be trained using a different objective function called "contrastive divergence" whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.},
    pages = {1771--1800},
    number = {8},
    journaltitle = {Neural Computation},
    shortjournal = {Neural Comput},
    author = {Hinton, Geoffrey E.},
    date = {2002-08},
    pmid = {12180402}
}


@article{wu_multimodal_2019,
    title = {Multimodal Generative Models for Compositional Representation Learning},
    url = {http://arxiv.org/abs/1912.05075},
    abstract = {As deep neural networks become more adept at traditional tasks, many of the most exciting new challenges concern multimodality---observations that combine diverse types, such as image and text. In this paper, we introduce a family of multimodal deep generative models derived from variational bounds on the evidence (data marginal likelihood). As part of our derivation we find that many previous multimodal variational autoencoders used objectives that do not correctly bound the joint marginal likelihood across modalities. We further generalize our objective to work with several types of deep generative model ({VAE}, {GAN}, and flow-based), and allow use of different model types for different modalities. We benchmark our models across many image, label, and text datasets, and find that our multimodal {VAEs} excel with and without weak supervision. Additional improvements come from use of {GAN} image models with {VAE} language models. Finally, we investigate the effect of language on learned image representations through a variety of downstream tasks, such as compositionally, bounding box prediction, and visual relation prediction. We find evidence that these image representations are more abstract and compositional than equivalent representations learned from only visual data.},
    journaltitle = {{arXiv}:1912.05075 [cs, stat]},
    author = {Wu, Mike and Goodman, Noah},
    urldate = {2021-09-11},
    date = {2019-12-10},
    eprinttype = {arxiv},
    eprint = {1912.05075},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}


@article{burgess_understanding_2018,
    title = {Understanding disentangling in \${\textbackslash}beta\$-{VAE}},
    url = {http://arxiv.org/abs/1804.03599},
    abstract = {We present new intuitions and theoretical assessments of the emergence of disentangled representation in variational autoencoders. Taking a rate-distortion theory perspective, we show the circumstances under which representations aligned with the underlying generative factors of variation of data emerge when optimising the modified {ELBO} bound in \${\textbackslash}beta\$-{VAE}, as training progresses. From these insights, we propose a modification to the training regime of \${\textbackslash}beta\$-{VAE}, that progressively increases the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in \${\textbackslash}beta\$-{VAE}, without the previous trade-off in reconstruction accuracy.},
    journaltitle = {{arXiv}:1804.03599 [cs, stat]},
    author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
    urldate = {2021-09-28},
    date = {2018-04-10},
    eprinttype = {arxiv},
    eprint = {1804.03599},
    note = {version: 1},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}


@article{dorent_hetero-modal_2019,
    title = {Hetero-Modal Variational Encoder-Decoder for Joint Modality Completion and Segmentation},
    url = {http://arxiv.org/abs/1907.11150},
    doi = {10.1007/978-3-030-32245-8_9},
    abstract = {We propose a new deep learning method for tumour segmentation when dealing with missing imaging modalities. Instead of producing one network for each possible subset of observed modalities or using arithmetic operations to combine feature maps, our hetero-modal variational 3D encoder-decoder independently embeds all observed modalities into a shared latent representation. Missing data and tumour segmentation can be then generated from this embedding. In our scenario, the input is a random subset of modalities. We demonstrate that the optimisation problem can be seen as a mixture sampling. In addition to this, we introduce a new network architecture building upon both the 3D U-Net and the Multi-Modal Variational Auto-Encoder ({MVAE}). Finally, we evaluate our method on {BraTS}2018 using subsets of the imaging modalities as input. Our model outperforms the current state-of-the-art method for dealing with missing modalities and achieves similar performance to the subset-specific equivalent networks.},
    journaltitle = {{arXiv}:1907.11150 [cs, eess]},
    author = {Dorent, Reuben and Joutard, Samuel and Modat, Marc and Ourselin, Sébastien and Vercauteren, Tom},
    urldate = {2021-09-28},
    date = {2019-07-25},
    eprinttype = {arxiv},
    eprint = {1907.11150},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}


@article{calixto_latent_2019,
    title = {Latent Variable Model for Multi-modal Translation},
    url = {http://arxiv.org/abs/1811.00357},
    abstract = {In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation ({MMT}) through a latent variable model. This latent variable can be seen as a multi-modal stochastic embedding of an image and its description in a foreign language. It is used in a target-language decoder and also to predict image features. Importantly, our model formulation utilises visual and textual inputs during training but does not require that images be available at test time. We show that our latent variable {MMT} formulation improves considerably over strong baselines, including a multi-task learning approach (Elliott and K{\textbackslash}'ad{\textbackslash}'ar, 2017) and a conditional variational auto-encoder approach (Toyama et al., 2016). Finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a constraint on the minimum amount of information encoded in the latent variable, and (iii) by training on additional target-language image descriptions (i.e. synthetic data).},
    journaltitle = {{arXiv}:1811.00357 [cs]},
    author = {Calixto, Iacer and Rios, Miguel and Aziz, Wilker},
    urldate = {2021-09-28},
    date = {2019-05-16},
    eprinttype = {arxiv},
    eprint = {1811.00357},
    keywords = {Computer Science - Computation and Language, I.2.7},

}


@article{ngiam_multimodal_nodate,
    title = {Multimodal Deep Learning},
    abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classiﬁer is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the {CUAVE} and {AVLetters} datasets on audio-visual speech classiﬁcation, demonstrating best published visual speech classiﬁcation on {AVLetters} and eﬀective shared representation learning.},
    pages = {8},
    author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
    langid = {english},
}


@incollection{daun_disent,
    location = {Cham},
    title = {Self-supervised Disentanglement of Modality-Specific and Shared Factors Improves Multimodal Generative Models},
    volume = {12544},
    isbn = {978-3-030-71277-8 978-3-030-71278-5},
    url = {https://link.springer.com/10.1007/978-3-030-71278-5_33},
    abstract = {Multimodal generative models learn a joint distribution over multiple modalities and thus have the potential to learn richer representations than unimodal models. However, current approaches are either ineﬃcient in dealing with more than two modalities or fail to capture both modality-speciﬁc and shared variations. We introduce a new multimodal generative model that integrates both modality-speciﬁc and shared factors and aggregates shared information across any subset of modalities eﬃciently. Our method partitions the latent space into disjoint subspaces for modality-speciﬁc and shared factors and learns to disentangle these in a purely self-supervised manner. Empirically, we show improvements in representation learning and generative performance compared to previous methods and showcase the disentanglement capabilities.},
    pages = {459--473},
    booktitle = {Pattern Recognition},
    publisher = {Springer International Publishing},
    author = {Daunhawer, Imant and Sutter, Thomas M. and Marcinkevičs, Ričards and Vogt, Julia E.},
    editor = {Akata, Zeynep and Geiger, Andreas and Sattler, Torsten},
    urldate = {2021-07-21},
    date = {2021},
    langid = {english},
    doi = {10.1007/978-3-030-71278-5_33},

}


@article{kingma_adam_2017,
    title = {Adam: A Method for Stochastic Optimization},
    url = {http://arxiv.org/abs/1412.6980},
    shorttitle = {Adam},
    abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
    journaltitle = {{arXiv}:1412.6980 [cs]},
    author = {Kingma, Diederik P. and Ba, Jimmy},
    urldate = {2021-10-09},
    date = {2017-01-29},
    eprinttype = {arxiv},
    eprint = {1412.6980},
    keywords = {Computer Science - Machine Learning},
}

